# strawberry-o1.lab
strawberry-o1: AGI from 0(zero) to 1(O1)

## Thinks 
- 这是最重要的问题吗？
- 为什么这样设计？
- 真的有效吗，严格的数学逻辑是什么？

## Notes
- 考虑对任意模型推理阶段加速


## Reference
《从零实现强化学习、RLHF、AlphaZero》-1：基于价值的强化学习1-理论基础、q learning、sarsa、dqn
https://zhuanlan.zhihu.com/p/673543350

Awesome-LLM-Strawberry
https://github.com/hijkzzz/Awesome-LLM-Strawberry?tab=readme-ov-file

Transformers from Scratch物理意义解释
https://e2eml.school/transformers.html#one_hot

How ChatGPT is fine-tuned using Reinforcement Learning
https://dida.do/blog/chatgpt-reinforcement-learning

How ChatGPT actually works
https://www.assemblyai.com/blog/how-chatgpt-actually-works/

OpenRLHF
https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/trainer/dpo_trainer.py

AlphaZero-Like Tree-Search can Guide
Large Language Model Decoding and Training
https://arxiv.org/pdf/2309.17179

Training Verifiers to Solve Math Word Problems
https://arxiv.org/pdf/2110.14168

Generative Language Modeling for Automated Theorem Proving
https://arxiv.org/abs/2009.03393

LLM Critics Help Catch LLM Bugs
https://arxiv.org/pdf/2407.00215

Self-critiquing models for assisting human evaluators
https://arxiv.org/pdf/2206.05802

Scalable Online Planning
via Reinforcement Learning Fine-Tuning
https://arxiv.org/pdf/2109.15316

Q*: Improving Multi-step Reasoning for LLMs with
Deliberative Planning https://arxiv.org/pdf/2406.14283

Let’s Verify Step by Step
https://arxiv.org/pdf/2305.20050

Don’t throw away your value model!
Generating more preferable text with Value-Guided Monte-Carlo
Tree Search decoding
https://arxiv.org/pdf/2309.15028
